{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorisation de matrices\n",
    "\n",
    "Auteurs : Slim Essid, Alexandre Gramfort, Joseph Salmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce travail vous devez déposer un **UNIQUE** ﬁchier sous format ipynb sous EOLE. Pour\n",
    "démarrer un ipython notebook sur les machines de l’école, utiliser la version appelée “(Anaconda)”\n",
    "dans l’onglet “Applications/développement/”. Le nom du ﬁchier uploadé sera OBLIGATOIREMENT\n",
    "Nom_Prenom_TP_Clustering.ipynb. Vous devrez charger votre ﬁchier sur Éole (SD207 > TPs > TP1),\n",
    "et ce avant 23h59, le 01/06/2017. La note totale est sur 20 points répartis comme suit :\n",
    " - qualité des réponses aux questions : 15 pts,\n",
    " - qualité de rédaction, de présentation et d’orthographe : 2 pts,\n",
    " - indentation, Style PEP8 (cf. par exemple https://github.com/ipython/ipython/wiki/Extensions-Index#pep8), commentaires adaptés : 2 pts,\n",
    " - absence de bug : 1 pt.\n",
    "Malus : 5 pts par tranche de 12h de retard (sauf excuses validées par l’administration); 2 pts pour non\n",
    "respect des autres consignes de rendu. La note sera ensuite ramenée sur 3 points dans la note ﬁnale de\n",
    "l’UE.\n",
    "**Rappel : aucun travail par mail accepté**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réduction de dimension par analyse en composantes principales (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprenons les images de chiffres manuscrits :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_digits: 10, \t n_samples 1797, \t n_features 64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_digits = len(np.unique(y))\n",
    "\n",
    "sample_size = 300\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n",
    "      % (n_digits, n_samples, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A faire :\n",
    "En utilisant sklearn, \n",
    " - afficher la base de données en dimension 2 après réduction de dimension de 64 à 2 avec une PCA ; \n",
    " - colorer les échantillons en fonction des vraies étiquettes ;\n",
    " - réaliser un clustering par K-moyennes de la base de données en utilisant les cractéristiques en 2D obtenues par PCA. \n",
    " - comparer au résultat obtenu précédemment sans réduction de dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction de caractéristiques faciales\n",
    "\n",
    "On envisage ici l'utilisation des techniques de décomposition par PCA et NMF pour l'extraction de caractéristiques utiles à la reconnaissance automatique de visages.\n",
    "\n",
    " - Étudier et tester le script ci-dessous. \n",
    "\n",
    " - Analyser le type de décomposition obtenu par NMF en comparaison avec celui obtenu par PCA. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par préparer les données et définir quelques fonctions utilitaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -- Prepare data and define utility functions \n",
    "\n",
    "n_row, n_col = 2, 5\n",
    "n_components = n_row * n_col\n",
    "image_shape = (64, 64)\n",
    "rng = RandomState(0)\n",
    "\n",
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces(data_home='c:/tmp/',shuffle=True,\n",
    "                               random_state=rng)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "faces_centered = faces - faces.mean(axis=0, dtype=np.float64)\n",
    "\n",
    "print \"Dataset consists of %d faces\" % n_samples\n",
    "\n",
    "def plot_gallery(title, images):\n",
    "    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=16)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "\n",
    "        comp = comp.reshape(image_shape)\n",
    "        vmax = comp.max()\n",
    "        vmin = comp.min()\n",
    "        dmy = np.nonzero(comp<0)\n",
    "        if len(dmy[0])>0:\n",
    "            yz, xz = dmy\n",
    "        comp[comp<0] = 0\n",
    "\n",
    "        plt.imshow(comp, cmap=plt.cm.gray, vmax=vmax, vmin=vmin)\n",
    "\n",
    "        if len(dmy[0])>0:\n",
    "            plt.plot( xz, yz, 'r,', hold=True)\n",
    "            print len(dmy[0]), \"negative-valued pixels\"\n",
    "\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "\n",
    "# Plot a sample of the input data\n",
    "plot_gallery(\"First centered Olivetti faces\",\n",
    "             faces_centered[:n_components])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit les méthodes à tester et leur paramétrisation, à compéter par vos soins :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- Decomposition methods \n",
    "\n",
    "# List of the different estimators and whether to center the data\n",
    "\n",
    "estimators = [\n",
    "    ('pca', 'Eigenfaces - PCA',\n",
    "     decomposition.PCA(<A_COMPLETER !!>]),\n",
    "     True),\n",
    "\n",
    "    ('nmf', 'Non-negative components - NMF',\n",
    "     decomposition.NMF(<A_COMPLETER !!>),\n",
    "     False)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et on applique à la base de données d'images :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -- Transform\n",
    "\n",
    "labels = dataset.target\n",
    "X = faces\n",
    "X_ = faces_centered\n",
    "\n",
    "for shortname, name, estimator, center in estimators:\n",
    "    #if shortname != 'nmf': continue\n",
    "    print \"Extracting the top %d %s...\" % (n_components, name)\n",
    "    t0 = time()\n",
    "\n",
    "    data = X\n",
    "    if center:\n",
    "        data = X_\n",
    "\n",
    "    data = estimator.fit_transform(data)\n",
    "\n",
    "    train_time = (time() - t0)\n",
    "    print \"done in %0.3fs\" % train_time\n",
    "\n",
    "    components_ = estimator.components_\n",
    "\n",
    "    plot_gallery('%s - Train time %.1fs' % (name, train_time),\n",
    "                 components_[:n_components])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A faire \n",
    "\n",
    "Modifier le script pour réaliser une évaluation des performances d'un système de reconnaissance automatique de visages utilisant les caractéristiques extraites par PCA, comparées à celles obtenues par un système exploitant les caractéristiques extraites par NMF. On pourra utiliser la LDA ou la régression logistique our la classification. On effectuera l'évaluation par validation croisée. On observera l'évolution des scores en faisant varier le nombre de composantes utilisé dans les décompositions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Extraction de thèmes à partir de textes\n",
    "\n",
    "Il s'agit dans cette partie de tester l'utilisation de la NMF pour l'extraction de thèmes à partir d'un corpus de textes ; l'idée principale étant d'interpréter chaque composante NMF extraite comme étant associée à un thème.\n",
    "\n",
    "Étudier et tester le script suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par définir quelques paramètres d'expérience : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "n_features = 900\n",
    "n_topics = 4\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " On charge les textes de certaines catégories du corpus [20newsgroups](http://qwone.com/~jason/20Newsgroups/) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# Load the 20 newsgroups dataset \n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# Load some categories from the training set\n",
    "categories = ['rec.sport.baseball','comp.graphics', 'comp.sys.mac.hardware',\n",
    "              'talk.religion.misc']\n",
    "\n",
    "print \"Loading dataset...\"\n",
    "dataset = datasets.fetch_20newsgroups(data_home='c:/data/text/', shuffle=True,\n",
    "                                      categories=categories, random_state=1)\n",
    "\n",
    "print \"done in %0.3fs.\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On représente chaque document par sa [TF-IDF](https://fr.wikipedia.org/wiki/TF-IDF) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vectorize data using the most common word\n",
    "# frequency with TF-IDF weighting (without top 5% stop words)\n",
    "\n",
    "print \"Extracting TF-IDF features...\"\n",
    "t0 = time()\n",
    "vectorizer = text.CountVectorizer(max_df=0.95, max_features=n_features, stop_words='english')\n",
    "counts = vectorizer.fit_transform(dataset.data[:n_samples])\n",
    "tfidf = text.TfidfTransformer().fit_transform(counts)\n",
    "print \"done in %0.3fs.\" % (time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique la NMF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the NMF model\n",
    "print \"Fitting the NMF model with n_samples=%d and n_features=%d...\" % (\n",
    "    n_samples, n_features)\n",
    "\n",
    "t0 = time()\n",
    "nmf = decomposition.NMF(n_components=n_topics).fit(tfidf)\n",
    "print \"done in %0.3fs.\" % (time() - t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et on interprète les \"vecteurs de base\" comme des thèmes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inverse the vectorizer vocabulary to be able\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print \"Topic #%d:\" % topic_idx\n",
    "    print \" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()[:-n_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La NMF permet donc de réaliser du clustering \"doux\" de documents : chaque document peut être associé à plusieurs thèmes à la fois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A faire\n",
    "\n",
    "Reproduire l'expérience précédente en utilisant des GMM et en interprétant les thèmes à partir des centroïdes de chaque cluster.\n",
    " - Comparer le résultat obtenu au résultat NMF.\n",
    " - En interprétant les thèmes au travers des mots clés les plus représentatifs, repérer les thèmes pour lesquels les deux méthodes (NMF et GMM) semblent réaliser un consensus.\n",
    " - En étiquettant chaque document par les 3 thèmes les plus pertinents, avec les deux approches, proposer une méthode qui permette de caractériser le \"taux d'accord\" de ces deux approches quant à l'étiquetage des documents. Calculer ce \"taux d'accord\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
